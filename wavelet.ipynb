{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pyarrow.parquet as pq\n",
    "from torch.autograd import Variable\n",
    "import pywt\n",
    "import pywt.data\n",
    "from scipy.signal import butter, lfilter \n",
    "from src.model import acl_net\n",
    "from src.utils import *\n",
    "#from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pq.read_pandas('../kaggle_data/train.parquet').to_pandas().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pro = signal_processing.SignalProcess()\n",
    "for j in range(5,6):\n",
    "    data = full_data[:,j * 1000:(j + 1) * 1000]\n",
    "    converted = np.apply_along_axis(sig_pro.butter_highpass_filter,0,data)\n",
    "    converted = np.apply_along_axis(sig_pro.denoise_signal, 0, converted[200000:])\n",
    "    #converted  = np.apply_along_axis(hard_thres,0,converted)\n",
    "    converted = np.apply_along_axis(corona_denoise,0,converted)\n",
    "    break\n",
    "    if j == 0:\n",
    "        convert = converted\n",
    "    else:\n",
    "        convert = np.hstack([convert,converted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(\"../ken_hayashima1989/metadata_train.csv\")[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = acl_net.ACLNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = np.vstack([converted[:204800,:],converted[-204800:,:]])\n",
    "agd = data_augmentor.AugmentedData(converted,meta_data)\n",
    "agd = DataLoader(agd,batch_size = 64,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for i ,sample_batched in enumerate(agd_data):\n",
    "        model.zero_grad()\n",
    "        #model.hidden = model.init_hidden()\n",
    "        signals = Variable(sample_batched['X'].float().to(device)) / 50\n",
    "        signals = signals.permute(0,2,1)\n",
    "        y_a = sample_batched['Y_A'].float().to(device)\n",
    "        y_b = sample_batched['Y_B'].float().to(device)\n",
    "        lam = sample_batched['lam'].float().to(device)\n",
    "        #print(signals.size())\n",
    "        #print(signals.size())\n",
    "\n",
    "        output = torch.squeeze(model(signals))\n",
    "        #print(output.size())\n",
    "        \n",
    "        #print(lam)\n",
    "        #y_a_loss = criterion(output, y_a)\n",
    "        #y_b_loss = criterion(output, y_b)\n",
    "        #print(y_a_loss)\n",
    "        loss = mixup_loss(output,y_a,y_b,lam)\n",
    "        #print(loss)\n",
    "        #writer.add_scalar('loss',loss)\n",
    "        if output.size()[0] != 64 and epoch % 5 == 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_loss(output,y_a,y_b,lam):\n",
    "    y_a_loss = y_a * torch.log(output + 1e-8) + (1 - y_a) * torch.log(1 - output + 1e-8)\n",
    "    y_b_loss = y_b * torch.log(output + 1e-8) + (1 - y_b) * torch.log(1 - output + 1e-8)\n",
    "    return - torch.mean(lam * y_a_loss + (1 - lam) * y_b_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pro = SignalProcess()\n",
    "for j in range(64):\n",
    "    validation = full_data[:,j * 100:(j + 1) * 100]\n",
    "    validation_converted = np.apply_along_axis(sig_pro.butter_highpass_filter,0,validation)\n",
    "    validation_converted = np.apply_along_axis(sig_pro.denoise_signal, 0, validation_converted[200000:])\n",
    "    validation_converted = np.apply_along_axis(corona_denoise,0,validation_converted)\n",
    "    converted = np.vstack([validation_converted[:204800,:],validation_converted[-204800:,:]])\n",
    "    converted = converted.T\n",
    "    converted = converted[:,None,:] \n",
    "    converted = torch.from_numpy(converted).float().to(device) / 50\n",
    "    output = torch.squeeze(model(converted)).cpu().data.numpy()\n",
    "    output = np.round(output).astype(int)\n",
    "    if j == 0:\n",
    "        outputs = output\n",
    "    else:\n",
    "        outputs = np.concatenate([outputs,output])\n",
    "ids = np.array(range(8712 + 14000,29049))\n",
    "test_output = pd.DataFrame({'signal_id':ids,'target':outputs})\n",
    "test_output.to_csv('final_ids.csv',index=None)\n",
    "    \n",
    "'''\n",
    "test = pq.read_pandas('../ken_hayashima1989/train.parquet').to_pandas().values\n",
    "for j in range(6):\n",
    "    #data = test[:,:(j + 1) * 500]\n",
    "    converted = np.apply_along_axis(sig_pro.butter_highpass_filter,0,test)\n",
    "    converted = np.apply_along_axis(sig_pro.denoise_signal, 0, converted[200000:])\n",
    "    #converted  = np.apply_along_axis(hard_thres,0,converted)\n",
    "    converted = np.apply_along_axis(corona_denoise,0,converted)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../ken_hayashima1989/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_id = pd.concat([pd.read_csv('first_7000_ids.csv'),pd.read_csv('second_7000_ids.csv'),pd.read_csv('final_ids.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_id.to_csv('ken_first_submission.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
